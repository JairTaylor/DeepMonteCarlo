{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search with Neural Networks applied to play Checkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jair Taylor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired in part by DeepMind's success with AlphaGo, we have written code that learns to play the game of Checkers using a somewhat similar methodology.  The algorithm uses a version of the Monte-Carlo Tree Search algorithm to learn find the proportion of wins that a good player should get from a given board state, and then trains a neural net to learn these probabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1.  Create framework for evaluating strength of play. e.g., given an algorithm, what is its strength in terms of budget for a tree?\n",
    "2.  Implement human-playable games.\n",
    "3.  Create more general structure for DNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    }
   ],
   "source": [
    "from montecarlo_lib import *  # Here we have implemented the Monte-Carlo Tree Search algorithm \n",
    "                              # as well as defining the rules of Checkers.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by using MCTS to generate a large number of games of checkers.  No human-played games are used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 commencing.\n",
      "Game is a draw.\n",
      "Game 1 commencing.\n",
      "Player 1 wins! (after 37 moves)\n",
      "Game 2 commencing.\n",
      "Game is a draw.\n",
      "Game 3 commencing.\n",
      "Player 1 wins! (after 49 moves)\n",
      "Game 4 commencing.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2438bd62358b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                        max_steps_to_simulate = 60)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUCTSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complete\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_actions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgame_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbudget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lodgiq/Documents/ml_stuff/github_repos/DeepMonteCarlo/montecarlo_lib.pyc\u001b[0m in \u001b[0;36mUCTSearch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m                         \u001b[0mgames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_game_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m                     \u001b[0mtotal_Q\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lodgiq/Documents/ml_stuff/github_repos/DeepMonteCarlo/montecarlo_lib.pyc\u001b[0m in \u001b[0;36mSimulate\u001b[0;34m(self, node, return_game_states)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mnsteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_game_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mgame_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lodgiq/Documents/ml_stuff/github_repos/DeepMonteCarlo/montecarlo_lib.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, i, inplace)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_from_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lodgiq/Documents/ml_stuff/github_repos/DeepMonteCarlo/montecarlo_lib.pyc\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, action, inplace)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_players\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_action_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lodgiq/Documents/ml_stuff/github_repos/DeepMonteCarlo/montecarlo_lib.pyc\u001b[0m in \u001b[0;36mupdate_action_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmove_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmove_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_legal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0mpossible_moves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lodgiq/Documents/ml_stuff/github_repos/DeepMonteCarlo/montecarlo_lib.pyc\u001b[0m in \u001b[0;36mis_legal\u001b[0;34m(self, action, explain)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Player %d trying to move a piece to outside of (0, %d)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "games_list = []\n",
    "winners_list = []\n",
    "all_game_trees_list = []\n",
    "max_turns = 50\n",
    "total_budget = 400\n",
    "num_games = 20\n",
    "\n",
    "for game_num in range(num_games):\n",
    "    game = checkers_state(board_size = 6, max_turns = max_turns, tiebreaker_rule = True)\n",
    "    #game.show_board()\n",
    "\n",
    "    game_states_list = []\n",
    "    game_trees_list = []\n",
    "\n",
    "    print \"Game %d commencing.\" % game_num\n",
    "    for i in range(max_turns):\n",
    "        num_actions = game.num_actions()     \n",
    "        game.notes = ''\n",
    "        game.notes += 'Turn %d: Player %d now choosing action from board above.\\n' %  (i, game.player)\n",
    "        game_to_play = deepcopy(game)\n",
    "        game_to_play.max_turns = 60 #This setting stops stalling-for-time strategies\n",
    "            \n",
    "        game_tree = MonteCarloTree( deepcopy(game_to_play), \n",
    "                       budget = total_budget, \n",
    "                       num_simulations = 1, \n",
    "                       max_steps_to_simulate = 60)\n",
    "        \n",
    "        action = game_tree.UCTSearch()\n",
    "\n",
    "        if not game_tree.root.is_complete and num_actions > 1 and len(game_tree.tree) < game_tree.budget:\n",
    "            raise ValueError(\"Game tree not fully built\")\n",
    "        game_trees_list.append(deepcopy(game_tree))\n",
    "        game.notes +=  'Size of tree: %d\\n' % len(game_tree.tree)\n",
    "\n",
    "        if action is None:\n",
    "            game.notes +=  \"No good move.  Taking random action.\\n\"\n",
    "            action = game.random_action()\n",
    "            \n",
    "        game_states_list.append(deepcopy(game))\n",
    "        (observation, reward, done, info) = game.step(action)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            winner = game.winner()\n",
    "            \n",
    "            if winner == 'draw':\n",
    "                note = \"Game is a draw.\"\n",
    "            else:\n",
    "                note = 'Player %d wins! (after %d moves)' % (game.winner(), i)\n",
    "            game.notes = note\n",
    "            print note\n",
    "            winners_list.append(game.winner())\n",
    "        #print game.notes\n",
    "        #game.show_board()\n",
    "        if done:\n",
    "            game_states_list.append(deepcopy(game))\n",
    "            break\n",
    "    else:\n",
    "        print \"Game timed out.\"\n",
    "        winners_list.append(None)\n",
    "        \n",
    "    games_list.append(deepcopy(game_states_list))\n",
    "    all_game_trees_list.append(game_trees_list)\n",
    "print winners_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assemble the training set.  The training set consists of all nodes (that is, game states) in all the trees created above with depth below a fixed max_depth, together with the win/lose/draw probabilities for Player 0 as an output to be learned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_Y = []\n",
    "\n",
    "for game_index in range(num_games):\n",
    "    game_states_list = games_list[game_index]\n",
    "    game_trees_list = all_game_trees_list[game_index]\n",
    "    \n",
    "    for turn_index in range(len(game_trees_list)):\n",
    "        game_tree = game_trees_list[turn_index]\n",
    "        for node in game_tree.tree.values():\n",
    "            \n",
    "            results = node.all_simulation_results\n",
    "            if len(results) > 0 and node.depth < 4:\n",
    "                num_victories = results.count(1)\n",
    "                num_losses = results.count(0)\n",
    "                num_draws = results.count(0.5)\n",
    "\n",
    "                if game_tree.root_player != 0:\n",
    "                    num_victories, num_losses = num_losses, num_victories\n",
    "                x = get_single_board_vector(node.state)\n",
    "                totes = float(len(results))\n",
    "                y = [num_victories/totes, num_losses/totes, num_draws/totes]\n",
    "                #each y is: proportion of player 0 victories, player 1 victories, draws\n",
    "                \n",
    "                all_X.append(x)\n",
    "                all_Y.append(y)\n",
    "    \n",
    "    \n",
    "train_indices = get_random_subset(len(all_X), .8)\n",
    "\n",
    "train_X = np.array([all_X[i] for i in range(len(all_X)) if i in train_indices])\n",
    "train_Y = np.array([all_Y[i] for i in range(len(all_X)) if i in train_indices])\n",
    "\n",
    "test_X = np.array([all_X[i] for i in range(len(all_X)) if i not in train_indices])\n",
    "test_Y = np.array([all_Y[i] for i in range(len(all_X)) if i not in train_indices])\n",
    "\n",
    "print 'train: %d / %d states (%.f pct)' % (len(train_X), len(all_X), 100 * len(train_X)/ float( len(all_X)))\n",
    "\n",
    "print 'test: %d / %d states (%.f pct)' % (len(test_X), len(all_X), 100 * len(test_X)/ float( len(all_X))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a neural net model using Keras to learn these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [30,20,10]\n",
    "activations = ['relu', 'relu', 'relu']\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    if activations is None:\n",
    "        activation = 'relu'\n",
    "    else:\n",
    "        activation = activations[i]\n",
    "\n",
    "    if i == 0:\n",
    "        model.add(Dense(layers[i],  activation = activation, input_dim = train_X.shape[1]))\n",
    "    else:\n",
    "        model.add(Dense(layers[i],  activation = activation))\n",
    "        \n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "model.fit(train_X, train_Y, epochs=1000, batch_size=100, verbose = 2)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_X, test_Y, verbose = 0)\n",
    "\n",
    "for i in range(len(model.metrics_names)):\n",
    "    \n",
    "    print \"%s on test set: %f\" % (model.metrics_names[i], scores[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the progress of a particular game of checkers and the associated win/lose/draw probabilites computed by our trained neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_to_view = 0\n",
    "game_states_list = games_list[game_to_view]\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "\n",
    "\n",
    "\n",
    "def game_slider(i):\n",
    "    game_states_list[i].show_board()\n",
    "    \n",
    "    v = get_single_board_vector(game_states_list[i])\n",
    "    X = np.array([v])\n",
    "    model_output = list(100 * model.predict(X)[0])\n",
    "    print game_states_list[i].notes\n",
    "    print \"Player 0 victory: \", model_output[0], '%'\n",
    "    print \"Player 1 victory: \", model_output[1], '%'\n",
    "    print \"Draw:             \", model_output[2], '%'\n",
    "    #return 'Player: %d' % game_states_list[i].player\n",
    "\n",
    "interact(game_slider, i = IntSlider(min=0,max=len(game_states_list)-1,step=1,value=0)  )\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a single step of a more general training algorithm to be implemented.  The idea is to simultaneously train the neural net while using the output from this model to improve the MCTS algorithm.\n",
    "\n",
    "- At each step, we have previously determined a weight $\\alpha \\in [0,1]$ and a neural net f that takes as input a board state $B$ and outputs a triple $(w,l,d)$ corresponding to its estimate of winning, losing or drawing assuming \"near-ideal\" play.\n",
    "- Play a large number of games using Monte-Carlo Tree Search.  \n",
    "- During backpropogation, two 3-tuples are backpropogated:\n",
    "    - $Q = (w, l, d)$ corresponding to the average number of wins, losses, and draws.\n",
    "    - $\\hat{Q}  = (\\hat{w}, \\hat{l}, \\hat{d}) = f(B)$ corresponding to output of the previously-neural net $f$.\n",
    "- For each node, set a value y corresponding to that node with $y = \\alpha Q + (1-\\alpha) \\hat{Q}$ for some fixed $\\alpha$. (Initially, set $\\alpha = 1$; all weight is given to $Q$ when neural net is not yet trained.)\n",
    "- The best child in MCTS is chosen according to the value of $y$.\n",
    "- After many games have been played, create training set for neural net consisting of:\n",
    "   - inputs $X$, game states corresponding to all nodes of depth < max_depth in all game trees\n",
    "   - outputs $Y$, the the $y$ values given to these game states.\n",
    "- Train neural net.\n",
    "- Periodically, re-set the value of $\\alpha$.  To find it, play a number of games between the following strategies: \n",
    "    - Strategy A: Choose moves by performing MCTS using random policy.\n",
    "    - Strategy B: Choose moves by performing MCTS using scoring by $f$.\n",
    "\n",
    "    Choose $\\alpha$ to be the proportion of games won by Strategy A (plus half the proportion of games drawn.)\n",
    "    \n",
    "If the algorithm is succeeding, the value of $\\alpha$ should gradually decrease on average.  Eventually, if $\\alpha$ becomes small enough, set $\\alpha = 0$ to save time (since evaluation of $\\hat{Q}$ should be much faster than evaluation of $Q$.)\n",
    "\n",
    "Periodically, the 'skill level' of neural net $f$ should be measured by the number of games won by Strategy A vs. Strategy B above, but with possibly with B given a handicap by having a large budget.  e.g., if $f$ is very accurate, Strategy A with a budget of 20 may be able to beat Strategy B with a budget of 100.\n",
    "\n",
    "\n",
    "\n",
    "Other notes / questions - \n",
    "\n",
    "- Should at first play with small board size to see if training is working\n",
    "- Over time, should increase the MCTS budget as $f$ becomes stronger\n",
    "- Should Q-values of game trees be shared across turns or games? If so, how?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
